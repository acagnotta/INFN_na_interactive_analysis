{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b420d95-518a-47c0-a450-3e2ae28bbcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import dask\n",
    "import os\n",
    "import time\n",
    "# Initialize ROOT\n",
    "# ROOT.PyConfig.IgnoreCommandLineOptions = True\n",
    "\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from distributed.diagnostics.plugin import UploadFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6250dd7a-5a5a-4267-a7a1-dc8bd3b895ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variable(object):\n",
    "    def __init__(self, name, title, nbins=None, xmin=None, xmax=None):\n",
    "        self._name = name\n",
    "        self._title = title\n",
    "        self._nbins = nbins\n",
    "        self._xmin = xmin\n",
    "        self._xmax = xmax\n",
    "    def __str__(self):\n",
    "        return  '\\\"'+str(self._name)+'\\\",\\\"'+str(self._title)+'\\\",\\\"'+str(self._nbins)+','+str(self._xmin)+','+str(self._xmax)\n",
    "\n",
    "my_vars = []\n",
    "\n",
    "my_vars.append(variable(name = \"e1_energy\", title= \"leading electron energy [GeV]\", nbins = 50, xmin = 0, xmax=100))\n",
    "my_vars.append(variable(name = \"e2_energy\", title= \"sub leading electron energy [GeV]\", nbins = 50, xmin = 0, xmax=100))\n",
    "my_vars.append(variable(name = \"m_ee\", title= \"Zee invariant mass, m_{ee} [GeV]\", nbins = 50, xmin = 84, xmax=98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3382bd74-fd21-42ea-b617-1d5058647384",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    \"\"\"\n",
    "    Setup connection to a Dask cluster. Two ingredients are needed:\n",
    "    1. Creating a cluster object that represents computing resources. This can be\n",
    "       done in various ways depending on the type of resources at disposal. To use\n",
    "       only the local machine (e.g. your laptop), a `LocalCluster` object can be\n",
    "       used. This step can be skipped if you have access to an existing Dask\n",
    "       cluster; in that case, the cluster administrator should provide you with a\n",
    "       URL to connect to the cluster in step 2. More options for cluster creation\n",
    "       can be found in the Dask docs at\n",
    "       http://distributed.dask.org/en/stable/api.html#cluster .\n",
    "    2. Creating a Dask client object that connects to the cluster. This accepts\n",
    "       directly the object previously created. In case the cluster was setup\n",
    "       externally, you need to provide an endpoint URL to the client, e.g.\n",
    "       'https://myscheduler.domain:8786'.\n",
    " \n",
    "    Through Dask, you can connect to various types of cluster resources. For\n",
    "    example, you can connect together a set of machines through SSH and use them\n",
    "    to run your computations. This is done through the `SSHCluster` class. For\n",
    "    example:\n",
    " \n",
    "    ```python\n",
    "    from dask.distributed import SSHCluster\n",
    "    cluster = SSHCluster(\n",
    "        # A list with machine host names, the first name will be used as\n",
    "        # scheduler, following names will become workers.\n",
    "        hosts=[\"machine1\",\"machine2\",\"machine3\"],\n",
    "        # A dictionary of options for each worker node, here we set the number\n",
    "        # of cores to be used on each node.\n",
    "        worker_options={\"nprocs\":4,},\n",
    "    )\n",
    "    ```\n",
    " \n",
    "    Another common usecase is interfacing Dask to a batch system like HTCondor or\n",
    "    Slurm. A separate package called dask-jobqueue (https://jobqueue.dask.org)\n",
    "    extends the available Dask cluster classes to enable running Dask computations\n",
    "    as batch jobs. In this case, the cluster object usually receives the parameters\n",
    "    that would be written in the job description file. For example:\n",
    " \n",
    "    ```python\n",
    "    from dask_jobqueue import HTCondorCluster\n",
    "    cluster = HTCondorCluster(\n",
    "        cores=1,\n",
    "        memory='2000MB',\n",
    "        disk='1000MB',\n",
    "    )\n",
    "    # Use the scale method to send as many jobs as needed\n",
    "    cluster.scale(4)\n",
    "    ```\n",
    " \n",
    "    In this tutorial, a cluster object is created for the local machine, using\n",
    "    multiprocessing (processes=True) on 4 workers (n_workers=4) each using only\n",
    "    1 core (threads_per_worker=1).\n",
    "    \"\"\"\n",
    "    cluster = LocalCluster(n_workers=2, threads_per_worker=1, processes=True)\n",
    "    client = Client(cluster)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fce237-f094-4904-b901-4156e57c5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables                                                                                                                                                \n",
    "fit_lowcut = 84.\n",
    "fit_highcut = 98.\n",
    "NbinsX = 50\n",
    "\n",
    "nmaxiteration = 200\n",
    "recreate_files= True\n",
    "\n",
    "\n",
    "sched_port = 25176#Dask port\n",
    "nmaxpartition = 10 # to set at lower value\n",
    "distributed = True#False#\n",
    "    \n",
    "folder = \"./output/mytest_Zee/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3de3d5-8af9-43a9-bf88-10a5de1c8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"functions.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc116a4-9e28-4cff-b66e-337cbfdcbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, var, nmaxiteration):\n",
    "    h_ = {}\n",
    "    \n",
    "    for i_sf in range(0,nmaxiteration):\n",
    "        for v in var:           \n",
    "            h_[v._name+\"_\"+str(i_sf)]= df.Histo1D(ROOT.RDF.TH1DModel(v._name+\"_\"+str(i_sf), v._title+\"; Events\", v._nbins, v._xmin, v._xmax), v._name+\"_\"+str(i_sf))# \n",
    "        \n",
    "    print(\"Done bookhisto!\")\n",
    "    return h_    \n",
    "\n",
    "\n",
    "def savehisto(h, var, nmaxiteration, repohisto):\n",
    "    label=\"m_ee_test\"\n",
    "    \n",
    "    Z_resolution = []\n",
    "    \n",
    "    if recreate_files== True:\n",
    "        outfile = ROOT.TFile.Open(repohisto+label+'.root', \"RECREATE\")\n",
    "    else:\n",
    "        outfile = ROOT.TFile.Open(repohisto+label+'.root', \"Update\")\n",
    "    for i_sf in range(0,nmaxiteration):\n",
    "        for v in var:\n",
    "            # print(h.keys())\n",
    "            tmp = h[v._name+\"_\"+str(i_sf)].GetValue()\n",
    "            outfile.cd()\n",
    "            tmp.Write()\n",
    "            tmp.Sumw2()\n",
    "            if v._name == \"Z_ee\":\n",
    "                Z_resolution.append(tmp.GetMean())\n",
    "    \n",
    "    outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5629b0ad-40fd-4bd3-837b-3fb613137b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/distributed/client.py:1128: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | client | scheduler | workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| msgpack | 1.0.3  | 1.0.2     | 1.0.3   |\n",
      "| toolz   | 0.11.2 | 0.11.1    | 0.11.2  |\n",
      "+---------+--------+-----------+---------+\n",
      "Notes: \n",
      "-  msgpack: Variation is ok, as long as everything is above 0.6\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# set up everything properly\n",
    "file_str = \"input_times_100.root\"\n",
    "file = \"/opt/workspace/persistent-storage/INFN_na_interactive_analysis/\"+file_str\n",
    "\n",
    "#ROOT.ROOT.EnableImplicitMT()\n",
    "\n",
    "if distributed == True:\n",
    "    from dask.distributed import Client\n",
    "    client = Client(\"localhost:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(file))\n",
    "    def set_proxy(dask_worker):\n",
    "        import shutil\n",
    "        working_dir = dask_worker.local_directory\n",
    "        shutil.copyfile(working_dir + '/'+file_str, working_dir + '/../../../'+file_str) \n",
    "        return working_dir\n",
    "    client.run(set_proxy)\n",
    "    # client.run(set_proxy)\n",
    "    # ROOT.ROOT.EnableImplicitMT()\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a84458a-7c72-4cba-9dbd-fb0034a7be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDataFrame that will use Dask as a backend for computations\n",
    "if distributed ==True:\n",
    "    #connection = create_connection()\n",
    "    df = RDataFrame(\"events\", file_str, npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\")\n",
    "else:\n",
    "    df = RDataFrame(\"events\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23e95aa-2f21-445b-a31a-4b80287391b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1_energy\n",
      "e2_energy\n",
      "m_ee\n",
      "Done bookhisto!\n",
      "Tempo impiegato in secondi:  159.25995349884033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class podio::ObjectID is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::ClusterData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::Vector3f is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::MCParticleData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::Vector3d is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::Vector2i is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::ReconstructedParticleData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::MCRecoParticleAssociationData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::ParticleIDData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::TrackData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::TrackState is available\n"
     ]
    }
   ],
   "source": [
    "var = my_vars\n",
    "\n",
    "for v in var:\n",
    "    print(v._name)\n",
    "\n",
    "df = df.Define('w_nominal', '1')\n",
    "df = df.Define(\"m_e\",\"0.0005124\") #GeV                                                                                                                           \n",
    "df_ge = df.Define(\"goodelectrons\", \"Particle.charge[0]*Particle.charge[1] < 0.\").Filter(\"goodelectrons > 0\")\n",
    "\n",
    "# Inizia a misurare il tempo\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "width_mass_mc = 2.49 #GeV                                                                                                                                        \n",
    "sigma_mass_mc = 2.6 #GeV                                                                                                                                         \n",
    "\n",
    "\n",
    "\n",
    "df_Mee = df_ge\n",
    "for i_sf in range(0,nmaxiteration):\n",
    "    # print(\"adding columns with i_sf=\", i_sf)\n",
    "    df_Mee = df_Mee.Define(\"m_ee_\"+str(i_sf), \"ComputeInvariantMass(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z, ComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e))\")\n",
    "\n",
    "    '''                                                                                                                                                          \n",
    "    che pesi usare?                                                                                                                                              \n",
    "    df = df.Define(\"w_nominal\",\"scaleFactor_ELECTRON * scaleFactor_ElectronTRIGGER * scaleFactor_PILEUP * mcWeight\");                                               \n",
    "    '''\n",
    "    # print(my_vars[0]._name+\"_\"+str(i_sf))\n",
    "    df_Mee = df_Mee.Define(my_vars[0]._name+\"_\"+str(i_sf),\"ComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e)[0]\")\n",
    "    df_Mee = df_Mee.Define(my_vars[1]._name+\"_\"+str(i_sf),\"ComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e)[1]\")\n",
    "\n",
    "tmp=bookhisto(df_Mee, var, nmaxiteration)\n",
    "savehisto(tmp, var, nmaxiteration, repohisto)\n",
    "\n",
    "# Termina la misurazione del tempo\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo trascorso\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Stampa il risultato\n",
    "print(\"Tempo impiegato in secondi: \", elapsed_time)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47f44d-5426-4631-8bdc-a702856e90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1a2bb-fea5-40dd-92df-c033de9c089f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93edba-f14c-4cfe-b5f4-9722dc903b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
