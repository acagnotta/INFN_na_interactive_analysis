{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b420d95-518a-47c0-a450-3e2ae28bbcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import dask\n",
    "import os\n",
    "import time\n",
    "# Initialize ROOT\n",
    "# ROOT.PyConfig.IgnoreCommandLineOptions = True\n",
    "\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from distributed.diagnostics.plugin import UploadFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6250dd7a-5a5a-4267-a7a1-dc8bd3b895ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variable(object):\n",
    "    def __init__(self, name, title, nbins=None, xmin=None, xmax=None):\n",
    "        self._name = name\n",
    "        self._title = title\n",
    "        self._nbins = nbins\n",
    "        self._xmin = xmin\n",
    "        self._xmax = xmax\n",
    "    def __str__(self):\n",
    "        return  '\\\"'+str(self._name)+'\\\",\\\"'+str(self._title)+'\\\",\\\"'+str(self._nbins)+','+str(self._xmin)+','+str(self._xmax)\n",
    "\n",
    "my_vars = []\n",
    "\n",
    "my_vars.append(variable(name = \"e1_energy\", title= \"leading electron energy [GeV]\", nbins = 50, xmin = 0, xmax=100))\n",
    "my_vars.append(variable(name = \"e2_energy\", title= \"sub leading electron energy [GeV]\", nbins = 50, xmin = 0, xmax=100))\n",
    "my_vars.append(variable(name = \"m_ee\", title= \"Zee invariant mass, m_{ee} [GeV]\", nbins = 50, xmin = 84, xmax=98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3382bd74-fd21-42ea-b617-1d5058647384",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    \"\"\"\n",
    "    Setup connection to a Dask cluster. Two ingredients are needed:\n",
    "    1. Creating a cluster object that represents computing resources. This can be\n",
    "       done in various ways depending on the type of resources at disposal. To use\n",
    "       only the local machine (e.g. your laptop), a `LocalCluster` object can be\n",
    "       used. This step can be skipped if you have access to an existing Dask\n",
    "       cluster; in that case, the cluster administrator should provide you with a\n",
    "       URL to connect to the cluster in step 2. More options for cluster creation\n",
    "       can be found in the Dask docs at\n",
    "       http://distributed.dask.org/en/stable/api.html#cluster .\n",
    "    2. Creating a Dask client object that connects to the cluster. This accepts\n",
    "       directly the object previously created. In case the cluster was setup\n",
    "       externally, you need to provide an endpoint URL to the client, e.g.\n",
    "       'https://myscheduler.domain:8786'.\n",
    " \n",
    "    Through Dask, you can connect to various types of cluster resources. For\n",
    "    example, you can connect together a set of machines through SSH and use them\n",
    "    to run your computations. This is done through the `SSHCluster` class. For\n",
    "    example:\n",
    " \n",
    "    ```python\n",
    "    from dask.distributed import SSHCluster\n",
    "    cluster = SSHCluster(\n",
    "        # A list with machine host names, the first name will be used as\n",
    "        # scheduler, following names will become workers.\n",
    "        hosts=[\"machine1\",\"machine2\",\"machine3\"],\n",
    "        # A dictionary of options for each worker node, here we set the number\n",
    "        # of cores to be used on each node.\n",
    "        worker_options={\"nprocs\":4,},\n",
    "    )\n",
    "    ```\n",
    " \n",
    "    Another common usecase is interfacing Dask to a batch system like HTCondor or\n",
    "    Slurm. A separate package called dask-jobqueue (https://jobqueue.dask.org)\n",
    "    extends the available Dask cluster classes to enable running Dask computations\n",
    "    as batch jobs. In this case, the cluster object usually receives the parameters\n",
    "    that would be written in the job description file. For example:\n",
    " \n",
    "    ```python\n",
    "    from dask_jobqueue import HTCondorCluster\n",
    "    cluster = HTCondorCluster(\n",
    "        cores=1,\n",
    "        memory='2000MB',\n",
    "        disk='1000MB',\n",
    "    )\n",
    "    # Use the scale method to send as many jobs as needed\n",
    "    cluster.scale(4)\n",
    "    ```\n",
    " \n",
    "    In this tutorial, a cluster object is created for the local machine, using\n",
    "    multiprocessing (processes=True) on 4 workers (n_workers=4) each using only\n",
    "    1 core (threads_per_worker=1).\n",
    "    \"\"\"\n",
    "    cluster = LocalCluster(n_workers=2, threads_per_worker=1, processes=True)\n",
    "    client = Client(cluster)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fce237-f094-4904-b901-4156e57c5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables                                                                                                                                                \n",
    "fit_lowcut = 84.\n",
    "fit_highcut = 98.\n",
    "NbinsX = 50\n",
    "\n",
    "nmaxiteration = 20\n",
    "recreate_files= True\n",
    "\n",
    "\n",
    "sched_port = 0 #Dask port\n",
    "nmaxpartition = 5 # to set at lower value\n",
    "distributed = True#False#\n",
    "    \n",
    "folder = \"./output/mytest_Zee/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3de3d5-8af9-43a9-bf88-10a5de1c8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"functions.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc116a4-9e28-4cff-b66e-337cbfdcbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, var, nmaxiteration):\n",
    "    h_ = {}\n",
    "    \n",
    "    for i_sf in range(0,nmaxiteration):\n",
    "        for v in var:           \n",
    "            h_[v._name+\"_\"+str(i_sf)]= df.Histo1D(ROOT.RDF.TH1DModel(v._name+\"_\"+str(i_sf), v._title+\"; Events\", v._nbins, v._xmin, v._xmax), v._name+\"_\"+str(i_sf))# \n",
    "        \n",
    "    print(\"Done bookhisto!\")\n",
    "    return h_    \n",
    "\n",
    "\n",
    "def savehisto(h, var, nmaxiteration, repohisto):\n",
    "    label=\"m_ee_test\"\n",
    "    \n",
    "    Z_resolution = []\n",
    "    \n",
    "    if recreate_files== True:\n",
    "        outfile = ROOT.TFile.Open(repohisto+label+'.root', \"RECREATE\")\n",
    "    else:\n",
    "        outfile = ROOT.TFile.Open(repohisto+label+'.root', \"Update\")\n",
    "    for i_sf in range(0,nmaxiteration):\n",
    "        for v in var:\n",
    "            # print(h.keys())\n",
    "            tmp = h[v._name+\"_\"+str(i_sf)].GetValue()\n",
    "            outfile.cd()\n",
    "            tmp.Write()\n",
    "            tmp.Sumw2()\n",
    "            if v._name == \"Z_ee\":\n",
    "                Z_resolution.append(tmp.GetMean())\n",
    "    \n",
    "    outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5629b0ad-40fd-4bd3-837b-3fb613137b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <EnableImplicitMT>: Cannot enable implicit multi-threading with 0 threads, please build ROOT with -Dimt=ON\n"
     ]
    }
   ],
   "source": [
    "# set up everything properly\n",
    "ROOT.ROOT.EnableImplicitMT()\n",
    "if distributed == True:\n",
    "    from dask.distributed import Client\n",
    "    sched_port = 11111\n",
    "    client = Client(\"localhost:\"+str(sched_port))\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a84458a-7c72-4cba-9dbd-fb0034a7be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDataFrame that will use Dask as a backend for computations\n",
    "file = \"/opt/workspace/persistent-storage/ee_Z_ee_EDM4Hep.root\"\n",
    "if distributed ==True:\n",
    "    #connection = create_connection()\n",
    "    df = RDataFrame(\"events\", file, npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\")\n",
    "else:\n",
    "    df = RDataFrame(\"events\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23e95aa-2f21-445b-a31a-4b80287391b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1_energy\n",
      "e2_energy\n",
      "m_ee\n",
      "Done bookhisto!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The number of requested partitions could be higher than the maximum amount of chunks the dataset can be split in. Some tasks could be doing no work. Consider setting the 'npartitions' parameter of the RDataFrame constructor to a lower value.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Failed to open file /opt/workspace/persistent-storage/ee_Z_ee_EDM4Hep.root",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     df_Mee \u001b[38;5;241m=\u001b[39m df_Mee\u001b[38;5;241m.\u001b[39mDefine(my_vars[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i_sf),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e)[1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m tmp\u001b[38;5;241m=\u001b[39mbookhisto(df_Mee, var, nmaxiteration)\n\u001b[0;32m---> 34\u001b[0m \u001b[43msavehisto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmaxiteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepohisto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Termina la misurazione del tempo\u001b[39;00m\n\u001b[1;32m     38\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36msavehisto\u001b[0;34m(h, var, nmaxiteration, repohisto)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_sf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,nmaxiteration):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# i_sf = str(i_sf)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#h[i_sf] = {}\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m var:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# print(h.keys())\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m         tmp \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi_sf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         outfile\u001b[38;5;241m.\u001b[39mcd()\n\u001b[1;32m     43\u001b[0m         tmp\u001b[38;5;241m.\u001b[39mWrite()\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Proxy.py:188\u001b[0m, in \u001b[0;36mActionProxy.GetValue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGetValue\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Returns the result value of the current action node if it was executed\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    before, else triggers the execution of the distributed graph before\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    returning the value.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mexecute_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxied_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxied_node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Proxy.py:55\u001b[0m, in \u001b[0;36mexecute_graph\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# If event-loop not triggered\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Creating a ROOT.TDirectory.TContext in a context manager so that\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# ROOT.gDirectory won't be changed by the event loop execution.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _managed_tcontext():\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# All the information needed to reconstruct the computation graph on\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m# the workers is contained in the head node\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m         \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/HeadNode.py:173\u001b[0m, in \u001b[0;36mHeadNode.execute_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m mapper \u001b[38;5;241m=\u001b[39m partial(distrdf_mapper,\n\u001b[1;32m    166\u001b[0m                  build_rdf_from_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_rdf_creator(),\n\u001b[1;32m    167\u001b[0m                  computation_graph_callable\u001b[38;5;241m=\u001b[39mcomputation_graph_callable,\n\u001b[1;32m    168\u001b[0m                  initialization_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minitialization,\n\u001b[1;32m    169\u001b[0m                  optimized\u001b[38;5;241m=\u001b[39moptimized)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Execute graph distributedly and return the aggregated results from all\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# tasks\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m returned_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcessAndMerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_ranges\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistrdf_reducer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Perform any extra checks that may be needed according to the\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# type of the head node\u001b[39;00m\n\u001b[1;32m    176\u001b[0m final_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_returned_values(returned_values)\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Backends/Dask/Backend.py:133\u001b[0m, in \u001b[0;36mDaskBackend.ProcessAndMerge\u001b[0;34m(self, ranges, mapper, reducer)\u001b[0m\n\u001b[1;32m    130\u001b[0m final_results \u001b[38;5;241m=\u001b[39m mergeables_lists\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m    131\u001b[0m progress(final_results)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinal_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:288\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:570\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    568\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 570\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/distributed/client.py:2722\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2720\u001b[0m         should_rejoin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2722\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/distributed/client.py:1977\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1976\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1980\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1983\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/distributed/client.py:865\u001b[0m, in \u001b[0;36mClient.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/distributed/utils.py:327\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    326\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/distributed/utils.py:310\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[0;32m--> 310\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     error[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tornado/gen.py:762\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/distributed/client.py:1842\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1842\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   1843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Backends/Dask/Backend.py:110\u001b[0m, in \u001b[0;36mdask_mapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m shared_libs_on_ex \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    105\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(localdir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filepath))\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m shared_libraries\n\u001b[1;32m    107\u001b[0m ]\n\u001b[1;32m    108\u001b[0m Utils\u001b[38;5;241m.\u001b[39mdeclare_shared_libraries(shared_libs_on_ex)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapper(current_range)\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Backends/Base.py:107\u001b[0m, in \u001b[0;36mdistrdf_mapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m setup_mapper(initialization_fn)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Build an RDataFrame instance for the current mapper task, based\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# on the type of the head node.\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m rdf_plus \u001b[38;5;241m=\u001b[39m build_rdf_from_range(current_range)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rdf_plus\u001b[38;5;241m.\u001b[39mrdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     mergeables \u001b[38;5;241m=\u001b[39m get_mergeable_values(rdf_plus\u001b[38;5;241m.\u001b[39mrdf, current_range\u001b[38;5;241m.\u001b[39mid, computation_graph_callable, optimized)\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/HeadNode.py:510\u001b[0m, in \u001b[0;36mbuild_rdf_from_range\u001b[0;34m()\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03mBuilds an RDataFrame instance for a distributed mapper.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03mThe function creates a TChain from the information contained in the\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03minput range object. If the chain cannot be built, returns None.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Prepare main TChain\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m chain, entries_in_trees \u001b[38;5;241m=\u001b[39m build_chain_from_range(current_range)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# The chain could not be built.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TaskObjects(\u001b[38;5;28;01mNone\u001b[39;00m, entries_in_trees)\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/HeadNode.py:470\u001b[0m, in \u001b[0;36mbuild_chain_from_range\u001b[0;34m()\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Build TChain of files for this range:\u001b[39;00m\n\u001b[1;32m    468\u001b[0m chain \u001b[38;5;241m=\u001b[39m ROOT\u001b[38;5;241m.\u001b[39mTChain(maintreename)\n\u001b[0;32m--> 470\u001b[0m clustered_range, entries_in_trees \u001b[38;5;241m=\u001b[39m Ranges\u001b[38;5;241m.\u001b[39mget_clustered_range_from_percs(current_range)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clustered_range \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# The task could not be correctly built, don't create the TChain\u001b[39;00m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, entries_in_trees\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Ranges.py:295\u001b[0m, in \u001b[0;36mget_clustered_range_from_percs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m trees_with_entries: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_n, (treename, filename, thistreepercstart, thistreepercend) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mzip\u001b[39m(percrange\u001b[38;5;241m.\u001b[39mtreenames, percrange\u001b[38;5;241m.\u001b[39mfilenames, treepercstarts, treepercends)):\n\u001b[1;32m    290\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# cluster boundary are included in the list. Example:\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     clusters, entries \u001b[38;5;241m=\u001b[39m get_clusters_and_entries(treename, filename)\n\u001b[1;32m    297\u001b[0m     fullpath \u001b[38;5;241m=\u001b[39m filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m treename\n\u001b[1;32m    298\u001b[0m     trees_with_entries[fullpath] \u001b[38;5;241m=\u001b[39m entries\n",
      "File \u001b[0;32m/usr/local/lib/root/DistRDF/Ranges.py:170\u001b[0m, in \u001b[0;36mget_clusters_and_entries\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_clusters_and_entries\u001b[39m(treename: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Retrieve cluster boundaries and number of entries of a TTree. If the tree\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    is empty, returns None, None.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     tfile \u001b[38;5;241m=\u001b[39m ROOT\u001b[38;5;241m.\u001b[39mTFile\u001b[38;5;241m.\u001b[39mOpen(filename)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tfile \u001b[38;5;129;01mor\u001b[39;00m tfile\u001b[38;5;241m.\u001b[39mIsZombie():\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/root/ROOT/_pythonization/_tfile.py:103\u001b[0m, in \u001b[0;36m_TFileOpen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m f \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m_OriginalOpen(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m==\u001b[39m bind_object(\u001b[38;5;241m0\u001b[39m, klass):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# args[0] can be either a string or a TFileOpenHandle\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to open file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mOSError\u001b[0m: Failed to open file /opt/workspace/persistent-storage/ee_Z_ee_EDM4Hep.root"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class podio::GenericParameters is available\n",
      "Warning in <TClass::Init>: no dictionary for class podio::ObjectID is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::ClusterData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::Vector3f is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::MCParticleData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::Vector3d is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::Vector2i is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::ReconstructedParticleData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::MCRecoParticleAssociationData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::ParticleIDData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::TrackData is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::TrackState is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm4hep::RecoParticleRefData is available\n",
      "Warning in <TClass::Init>: no dictionary for class podio::CollectionIDTable is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<int,podio::GenericParameters> is available\n"
     ]
    }
   ],
   "source": [
    "var = my_vars\n",
    "\n",
    "for v in var:\n",
    "    print(v._name)\n",
    "\n",
    "df = df.Define('w_nominal', '1')\n",
    "df = df.Define(\"m_e\",\"0.0005124\") #GeV                                                                                                                           \n",
    "df_ge = df.Define(\"goodelectrons\", \"Particle.charge[0]*Particle.charge[1] < 0.\").Filter(\"goodelectrons > 0\")\n",
    "\n",
    "# Inizia a misurare il tempo\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "width_mass_mc = 2.49 #GeV                                                                                                                                        \n",
    "sigma_mass_mc = 2.6 #GeV                                                                                                                                         \n",
    "\n",
    "\n",
    "\n",
    "df_Mee = df_ge\n",
    "for i_sf in range(0,nmaxiteration):\n",
    "    # print(\"adding columns with i_sf=\", i_sf)\n",
    "    df_Mee = df_Mee.Define(\"m_ee_\"+str(i_sf), \"ComputeInvariantMass(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z, ComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e))\")\n",
    "\n",
    "    '''                                                                                                                                                          \n",
    "    che pesi usare?                                                                                                                                              \n",
    "    df = df.Define(\"w_nominal\",\"scaleFactor_ELECTRON * scaleFactor_ElectronTRIGGER * scaleFactor_PILEUP * mcWeight\");                                               \n",
    "    '''\n",
    "    # print(my_vars[0]._name+\"_\"+str(i_sf))\n",
    "    df_Mee = df_Mee.Define(my_vars[0]._name+\"_\"+str(i_sf),\"ComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e)[0]\")\n",
    "    df_Mee = df_Mee.Define(my_vars[1]._name+\"_\"+str(i_sf),\"ComputeEnergy(Particle.momentum.x, Particle.momentum.y, Particle.momentum.z,m_e)[1]\")\n",
    "\n",
    "tmp=bookhisto(df_Mee, var, nmaxiteration)\n",
    "savehisto(tmp, var, nmaxiteration, repohisto)\n",
    "\n",
    "# Termina la misurazione del tempo\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo trascorso\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Stampa il risultato\n",
    "print(\"Tempo impiegato in secondi: \", elapsed_time)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47f44d-5426-4631-8bdc-a702856e90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93edba-f14c-4cfe-b5f4-9722dc903b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
